# sagemaker-mlops-lab
A hands-on lab collection of small, real-world machine learning projects using Amazon SageMakerâ€”covering data preparation, training, tuning, deployment, and model monitoring


# Initial setup
```bash
mkdir -p traffic/{1_data_preparation,2_model_training,3_model_tuning,4_model_deployment,5_model_monitoring,notebooks,flows} && \
touch traffic/README.md traffic/requirements.txt && \
touch traffic/1_data_preparation/{feature_store_ingest.py,traffic_data.csv} && \
touch traffic/2_model_training/{xgb_train_from_featurestore.py,train.csv,validation.csv} && \
touch traffic/3_model_tuning/{hyperparameter_tuning_job.py,config.yaml} && \
touch traffic/4_model_deployment/{deploy_model.py,predictor_test.py} && \
touch traffic/5_model_monitoring/{model_monitor_setup.py,baseline_constraints.json} && \
touch traffic/notebooks/{FeatureStore.ipynb,xgb-training.ipynb} && \
touch traffic/flows/FeatureStoreExport.flow
```

## ğŸ”§ Project Modules

| Module | Description |
|--------|-------------|
| `1_data_preparation/` | Create feature groups and ingest datasets to SageMaker Feature Store |
| `2_model_training/`   | Train an XGBoost model using built-in algorithms and Feature Store queries |
| `3_model_tuning/`     | Run SageMaker hyperparameter tuning jobs for better model performance |
| `4_model_deployment/` | Deploy the model to a real-time endpoint and invoke it |
| `5_model_monitoring/` | Enable data capture, baseline monitoring, and drift detection |

---

# Option 1: Local development using Boto3 + SageMaker SDK:
- ğŸ“ traffic/1_data_preparation/feature_store_ingest.py
    - âœ… Reads traffic_data.csv
    - âœ… Converts timestamp to ISO format
    - âœ… Creates a Feature Group in SageMaker
    - âœ… Uploads all records to the Feature Store

- ğŸ“ traffic/2_model_training/xgb_train_from_featurestore.py
    - âœ… Queries the Feature Store via Athena
    - âœ… Loads and cleans data
    - âœ… Splits data for training
    - âœ… Trains XGBoost with ml.m5.large
    - âœ… Uploads training CSV to S3

- ğŸ“ traffic/3_model_tuning/hyperparameter_tuning_job.py
    - âœ… Defines a tuning job using SageMakerâ€™s HPO
    - âœ… Optimizes max_depth, eta, gamma, etc.
    - âœ… Metric: validation:auc
    - âœ… Launches max_jobs=10, parallel=2

- ğŸ“ traffic/4_model_deployment/deploy_model.py
    - âœ… Loads model artifact from S3
    - âœ… Deploys as a real-time SageMaker endpoint

- ğŸ“ traffic/4_model_deployment/predictor_test.py
    - âœ… Reads validation.csv
    - âœ… Sends sample rows for prediction
    - âœ… Prints predictions from the endpoint

- ğŸ“ traffic/5_model_monitoring/model_monitor_setup.py
    - âœ… Enables data capture on deployed endpoint
    - âœ… Stores requests/responses in S3
    - âœ… Prepares for future baseline drift detection


# ğŸ“Œ List of python packages:
- âœ… boto3: Required for AWS client access (e.g., S3, SageMaker Feature Store)
- âœ… sagemaker: AWS SageMaker Python SDK
- âœ… pandas: For CSV handling, dataframes, transformations
- âœ… scikit-learn: For train/test split and metrics
- âœ… xgboost: For model training using built-in or local mode
- âœ… python-dotenv: For loading .env configurations


# Setup python virtual environment
```bash
@btholath âœ /workspaces/sagemaker-mlops-lab (main) $ python -m venv .venv
@btholath âœ /workspaces/sagemaker-mlops-lab (main) $ source .venv/bin/activate
@btholath âœ /workspaces/sagemaker-mlops-lab (main) $ pip install --upgrade pip
(.venv) @btholath âœ /workspaces/sagemaker-mlops-lab (main) $ pip install -r ./traffic/requirements.txt
```

# cleanup aws resources
```bash
aws sagemaker delete-feature-group --feature-group-name traffic-feature-group-local
aws sagemaker list-feature-groups
```        

# Setup AWS CLI
```bash
(.venv) @btholath âœ /workspaces
(.venv) @btholath âœ /workspacescurl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
(.venv) @btholath âœ /workspacesunzip awscliv2.zip
(.venv) @btholath âœ /workspacessudo ./aws/install
or
(.venv) @btholath âœ /workspaces./aws/install -i ~/.local/aws-cli -b ~/.local/bin
export PATH=$PATH:~/.local/bin
(.venv) @btholath âœ /workspaces/usr/local/bin/aws --version 
(.venv) @btholath âœ /workspaces $ aws configure
(.venv) @btholath âœ /workspaces $ aws sts get-caller-identity
```

# Pre-requisites
- Global Setup (Before Any Script)
    - 1. Environment
    -   Python 3.8+ installed
    - Virtual environment activated (python -m venv .venv && source .venv/bin/activate)
      Required packages installed: pip install -r traffic/requirements.txt

- 2 .env File Present at traffic/.env
    AWS_REGION=us-west-2
    SAGEMAKER_ROLE=arn:aws:iam::<your-account>:role/<SageMakerExecutionRole>
    S3_BUCKET=sagemaker-traffic-prediction-bucket
    S3_PREFIX=traffic-pipeline
    FEATURE_GROUP_NAME=traffic-feature-group-local
    ENDPOINT_NAME=xgboost-traffic-local-endpoint

- 3. IAM Role Permissions (SAGEMAKER_ROLE)
    Make sure the IAM role has access to:
    AmazonS3FullAccess
    AmazonSageMakerFullAccess
    AthenaFullAccess
    GlueFullAccess (for Feature Store query)

#ğŸš¦ Script-wise Prerequisites
# ğŸ“ 1_data_preparation/feature_store_ingest.py
 - âœ… Before Running:
    - File traffic/1_data_preparation/traffic_data.csv must exist.
    - Ensure the FEATURE_GROUP_NAME doesn't already exist (or handle overwrite).
    - Feature Store + Athena + Glue permissions granted.

# ğŸ“ 2_model_training/xgb_train_from_featurestore.py
- âœ… Before Running:
    - feature_store_ingest.py has already run and populated the Feature Group.
    - Athena has permission to read query results.
    - .env variables are valid and consistent.

# ğŸ“ 3_model_tuning/hyperparameter_tuning_job.py
- âœ… Before Running:
    - xgb_train_from_featurestore.py has generated and uploaded train.csv to S3.
    - S3 path s3://$S3_BUCKET/$S3_PREFIX/train/train.csv exists.
    - IAM Role has SageMakerFullAccess.

# ğŸ“ 4_model_deployment/deploy_model.py
- âœ… Before Running:
    - xgb_train_from_featurestore.py or tuning job has produced a model artifact at:
    - s3://$S3_BUCKET/$S3_PREFIX/output/xgboost-traffic-local/output/model.tar.gz

# ğŸ“ 4_model_deployment/predictor_test.py
- âœ… Before Running:
    - Endpoint specified in ENDPOINT_NAME has been deployed and is InService.
    - File traffic/2_model_training/validation.csv exists and contains feature-aligned samples.

# ğŸ“ 5_model_monitoring/model_monitor_setup.py
- âœ… Before Running:
    - A live endpoint already exists.
    - You want to start logging inference data.
    - S3 destination in .env is ready for data capture.


# To delete all AWS resources created for your traffic prediction project, you need to clean up:
- âœ… SageMaker Feature Group
- âœ… S3 data artifacts
- âœ… SageMaker Model, Endpoint, Endpoint Configuration
- âœ… SageMaker Training Jobs, Tuning Jobs (optional cleanup)
- âœ… Athena query results (optional)

# ğŸ§¨ Option 1: AWS CLI Commands
# ğŸ”¥ Delete SageMaker Endpoint and Config
```bash
aws sagemaker delete-endpoint --endpoint-name xgboost-traffic-local-endpoint
aws sagemaker delete-endpoint-config --endpoint-config-name xgboost-traffic-local-endpoint
```

# ğŸ”¥ Delete SageMaker Model
```bash
aws sagemaker delete-model --model-name xgboost-traffic-local-endpoint
```
# ğŸ”¥ Delete SageMaker Feature Group
```bash
aws sagemaker delete-feature-group --feature-group-name traffic-feature-group-local
```
ğŸ•’ This may take time if the Feature Store is still ingesting data.

# ğŸ”¥ Delete S3 Bucket Contents
```bash
aws s3 rm s3://sagemaker-traffic-prediction-bucket/ --recursive
```
Replace with your actual bucket from .env.